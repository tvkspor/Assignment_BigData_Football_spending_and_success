{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88142c6b-66e0-49d3-8ce2-a68b257e9623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_appearances = spark.read.csv(\"/FileStore/tables/appearances.csv\", header=True, inferSchema=True)\n",
    "df_clubs = spark.read.csv(\"/FileStore/tables/clubs.csv\", header=True, inferSchema=True)\n",
    "df_transfers = spark.read.csv(\"/FileStore/tables/transfers.csv\", header=True, inferSchema=True)\n",
    "df_games = spark.read.csv(\"/FileStore/tables/games.csv\", header=True, inferSchema=True)\n",
    "df_players = spark.read.csv(\"/FileStore/tables/players.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974328bf-1a74-402a-9b0d-ea98540426a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Danh sách các bảng cần cập nhật cột ngày\n",
    "tables = [\"appearances\", \"games\",]\n",
    "\n",
    "for table in tables:\n",
    "    df = spark.read.csv(f\"/FileStore/tables/{table}.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Chuyển đổi số nguyên thành ngày (epoch-based)\n",
    "    df = df.withColumn(\"date\", F.expr(\"date_add('1970-01-01', CAST(date AS INT))\"))\n",
    "    \n",
    "    # Lưu lại dữ liệu sau khi xử lý\n",
    "    df.write.mode(\"overwrite\").parquet(f\"dbfs:/tmp/{table}.parquet\")\n",
    "\n",
    "# Xử lý bảng transfers riêng (có cột transfer_date)\n",
    "df_transfers = spark.read.csv(\"/FileStore/tables/transfers.csv\", header=True, inferSchema=True)\n",
    "df_transfers = df_transfers.withColumn(\"transfer_date\", F.expr(\"date_add('1970-01-01', CAST(transfer_date AS INT))\"))\n",
    "df_transfers.write.mode(\"overwrite\").parquet(\"dbfs:/tmp/transfers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dc6fe8-731a-4ef1-a8e2-0941e4d02297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Đọc dữ liệu từ DBFS\n",
    "df_appearances = spark.read.csv(\"/FileStore/tables/appearances.csv\", header=True, inferSchema=True)\n",
    "df_games = spark.read.csv(\"/FileStore/tables/games.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Đổi tên cột 'competition_id' trong df_games để tránh xung đột\n",
    "df_games = df_games.withColumnRenamed(\"competition_id\", \"competition_id_game\")\n",
    "df_games = df_games.withColumnRenamed(\"date\", \"game_date\")\n",
    "\n",
    "# Thực hiện join hai bảng dựa trên game_id\n",
    "df_appearances2 = df_appearances.join(df_games, on=\"game_id\", how=\"inner\") \\\n",
    "    .withColumn(\"away_home\", \n",
    "                F.when(F.col(\"player_club_id\") == F.col(\"home_club_id\"), \"home\")\n",
    "                 .otherwise(\"away\")) \\\n",
    "    .withColumn(\"result\", \n",
    "                F.when((F.col(\"player_club_id\") == F.col(\"home_club_id\")) & (F.col(\"home_club_goals\") > F.col(\"away_club_goals\")), \"W\")\n",
    "                 .when((F.col(\"player_club_id\") == F.col(\"home_club_id\")) & (F.col(\"home_club_goals\") == F.col(\"away_club_goals\")), \"D\")\n",
    "                 .when((F.col(\"player_club_id\") == F.col(\"home_club_id\")) & (F.col(\"home_club_goals\") < F.col(\"away_club_goals\")), \"L\")\n",
    "                 .when((F.col(\"player_club_id\") == F.col(\"away_club_id\")) & (F.col(\"away_club_goals\") > F.col(\"home_club_goals\")), \"W\")\n",
    "                 .when((F.col(\"player_club_id\") == F.col(\"away_club_id\")) & (F.col(\"away_club_goals\") == F.col(\"home_club_goals\")), \"D\")\n",
    "                 .otherwise(\"L\")) \\\n",
    "    .withColumn(\"score\", F.concat_ws(\"-\", F.col(\"home_club_goals\"), F.col(\"away_club_goals\"))) \\\n",
    "    .withColumn(\"GF\", \n",
    "                F.when(F.col(\"player_club_id\") == F.col(\"home_club_id\"), F.col(\"home_club_goals\"))\n",
    "                 .otherwise(F.col(\"away_club_goals\"))) \\\n",
    "    .withColumn(\"GA\", \n",
    "                F.when(F.col(\"player_club_id\") == F.col(\"home_club_id\"), F.col(\"away_club_goals\"))\n",
    "                 .otherwise(F.col(\"home_club_goals\"))) \\\n",
    "    .withColumn(\"manager_name\", \n",
    "                F.when(F.col(\"player_club_id\") == F.col(\"home_club_id\"), F.col(\"home_club_manager_name\"))\n",
    "                 .otherwise(F.col(\"away_club_manager_name\"))) \\\n",
    "    .withColumn(\"attendance\", F.col(\"attendance\"))\n",
    "\n",
    "# Lưu bảng appearances2 dưới dạng Parquet\n",
    "df_appearances2.write.mode(\"overwrite\").parquet(\"dbfs:/tmp/appearances2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932a6d40-255c-4298-b933-5c5511571411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read in the games and clubs data\n",
    "df_games = spark.read.csv(\"/FileStore/tables/games.csv\", header=True, inferSchema=True)\n",
    "df_clubs = spark.read.parquet(\"dbfs:/tmp/clubs.parquet\")\n",
    "\n",
    "# Calculate home game stats\n",
    "home_games = df_games.filter(F.col(\"home_club_name\").isNotNull()).groupBy(\n",
    "    \"season\", \"home_club_id\", \"home_club_name\"\n",
    ").agg(\n",
    "    F.count(\"game_id\").alias(\"total_games_home\"),\n",
    "    F.sum(F.when(F.col(\"home_club_goals\") > F.col(\"away_club_goals\"), 1).otherwise(0)).alias(\"win_home\"),\n",
    "    F.sum(F.when(F.col(\"home_club_goals\") == F.col(\"away_club_goals\"), 1).otherwise(0)).alias(\"draw_home\"),\n",
    "    F.sum(F.when(F.col(\"home_club_goals\") < F.col(\"away_club_goals\"), 1).otherwise(0)).alias(\"lost_home\"),\n",
    "    F.sum(\"home_club_goals\").alias(\"GF_home\"),\n",
    "    F.sum(\"away_club_goals\").alias(\"GA_home\")\n",
    ")\n",
    "\n",
    "# Calculate away game stats\n",
    "away_games = df_games.filter(F.col(\"away_club_name\").isNotNull()).groupBy(\n",
    "    \"season\", \"away_club_id\", \"away_club_name\"\n",
    ").agg(\n",
    "    F.count(\"game_id\").alias(\"total_games_away\"),\n",
    "    F.sum(F.when(F.col(\"away_club_goals\") > F.col(\"home_club_goals\"), 1).otherwise(0)).alias(\"win_away\"),\n",
    "    F.sum(F.when(F.col(\"away_club_goals\") == F.col(\"home_club_goals\"), 1).otherwise(0)).alias(\"draw_away\"),\n",
    "    F.sum(F.when(F.col(\"away_club_goals\") < F.col(\"home_club_goals\"), 1).otherwise(0)).alias(\"lost_away\"),\n",
    "    F.sum(\"away_club_goals\").alias(\"GF_away\"),\n",
    "    F.sum(\"home_club_goals\").alias(\"GA_away\")\n",
    ")\n",
    "\n",
    "home_games = home_games.alias(\"home\")\n",
    "away_games = away_games.alias(\"away\")\n",
    "\n",
    "# Join home and away stats on season and club_id\n",
    "team_stats = home_games.join(\n",
    "    away_games, \n",
    "    (F.col(\"home.season\") == F.col(\"away.season\")) & (F.col(\"home.home_club_id\") == F.col(\"away.away_club_id\")),\n",
    "    how=\"outer\"\n",
    ").select(\n",
    "    F.coalesce(F.col(\"home.season\"), F.col(\"away.season\")).alias(\"season\"),\n",
    "    F.coalesce(F.col(\"home.home_club_id\"), F.col(\"away.away_club_id\")).alias(\"club_id\"),\n",
    "    F.coalesce(F.col(\"home.home_club_name\"), F.col(\"away.away_club_name\")).alias(\"club_name\"),\n",
    "    \n",
    "    (F.coalesce(F.col(\"home.total_games_home\"), F.lit(0)) + F.coalesce(F.col(\"away.total_games_away\"), F.lit(0))).alias(\"total_games\"),\n",
    "    (F.coalesce(F.col(\"home.win_home\"), F.lit(0)) + F.coalesce(F.col(\"away.win_away\"), F.lit(0))).alias(\"win\"),\n",
    "    (F.coalesce(F.col(\"home.draw_home\"), F.lit(0)) + F.coalesce(F.col(\"away.draw_away\"), F.lit(0))).alias(\"draw\"),\n",
    "    (F.coalesce(F.col(\"home.lost_home\"), F.lit(0)) + F.coalesce(F.col(\"away.lost_away\"), F.lit(0))).alias(\"lost\"),\n",
    "\n",
    "    # Corrected performance formula\n",
    "    F.round(\n",
    "        (3.0 * (F.coalesce(F.col(\"home.win_home\"), F.lit(0)) + F.coalesce(F.col(\"away.win_away\"), F.lit(0))) +\n",
    "         (F.coalesce(F.col(\"home.draw_home\"), F.lit(0)) + F.coalesce(F.col(\"away.draw_away\"), F.lit(0)))) /\n",
    "        (F.coalesce(F.col(\"home.total_games_home\"), F.lit(0)) + F.coalesce(F.col(\"away.total_games_away\"), F.lit(0))) * 100 /3, 2\n",
    "    ).alias(\"performance\"),\n",
    "\n",
    "    # Corrected GF, GA, GD calculations\n",
    "    (F.coalesce(F.col(\"home.GF_home\"), F.lit(0)) + F.coalesce(F.col(\"away.GF_away\"), F.lit(0))).alias(\"GF\"),\n",
    "    (F.coalesce(F.col(\"home.GA_home\"), F.lit(0)) + F.coalesce(F.col(\"away.GA_away\"), F.lit(0))).alias(\"GA\"),\n",
    "    F.round(\n",
    "        (F.coalesce(F.col(\"home.GF_home\"), F.lit(0)) + F.coalesce(F.col(\"away.GF_away\"), F.lit(0))) -\n",
    "        (F.coalesce(F.col(\"home.GA_home\"), F.lit(0)) + F.coalesce(F.col(\"away.GA_away\"), F.lit(0))), 2\n",
    "    ).alias(\"GD\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add club info\n",
    "club_info = df_clubs.select(\"club_id\", \"domestic_competition_id\")\n",
    "\n",
    "team_stats = team_stats.join(\n",
    "    club_info, team_stats[\"club_id\"] == club_info[\"club_id\"], how=\"left\"\n",
    ").drop(club_info[\"club_id\"])\n",
    "\n",
    "\n",
    "# Save to Parquet or insert into database\n",
    "team_stats.write.mode(\"overwrite\").parquet(\"dbfs:/tmp/team_stats.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc047901-544b-4069-ad98-ddf7f3b21036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Read the transfer data from CSV\n",
    "df_transfers = spark.read.csv(\"/FileStore/tables/transfers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Clean the data: Convert 'transfer_fee' and 'market_value_in_eur' columns to numeric values\n",
    "df_transfers = df_transfers.withColumn(\n",
    "    \"transfer_fee\", F.coalesce(F.col(\"transfer_fee\").cast(\"double\"), F.lit(0))\n",
    ").withColumn(\n",
    "    \"market_value_in_eur\", F.coalesce(F.col(\"market_value_in_eur\").cast(\"double\"), F.lit(0))\n",
    ")\n",
    "\n",
    "# Grouping data for outgoing transfers (from_club_id)\n",
    "transfer_summary_out = df_transfers.groupBy(\"transfer_season\", \"from_club_id\").agg(\n",
    "    F.sum(F.when(F.col(\"transfer_fee\").isNotNull(), F.col(\"transfer_fee\")).otherwise(0)).alias(\"total_transfer_fee_out\"),\n",
    "    F.sum(F.when(F.col(\"market_value_in_eur\").isNotNull(), F.col(\"market_value_in_eur\")).otherwise(0)).alias(\"total_market_value_out\")\n",
    ")\n",
    "\n",
    "# Grouping data for incoming transfers (to_club_id)\n",
    "transfer_summary_in = df_transfers.groupBy(\"transfer_season\", \"to_club_id\").agg(\n",
    "    F.sum(F.when(F.col(\"transfer_fee\").isNotNull(), F.col(\"transfer_fee\")).otherwise(0)).alias(\"total_transfer_fee_in\"),\n",
    "    F.sum(F.when(F.col(\"market_value_in_eur\").isNotNull(), F.col(\"market_value_in_eur\")).otherwise(0)).alias(\"total_market_value_in\")\n",
    ")\n",
    "\n",
    "# Renaming columns for consistency\n",
    "transfer_summary_out = transfer_summary_out.withColumnRenamed(\"from_club_id\", \"club_id\")\n",
    "transfer_summary_in = transfer_summary_in.withColumnRenamed(\"to_club_id\", \"club_id\")\n",
    "\n",
    "# Merging the two summaries (outgoing and incoming transfers)\n",
    "transfer_summary = transfer_summary_out.join(\n",
    "    transfer_summary_in,\n",
    "    on=[\"transfer_season\", \"club_id\"],\n",
    "    how=\"outer\"\n",
    ").fillna(0)\n",
    "\n",
    "# Calculating total transfer fee (subtracting outgoing transfer fees and adding incoming)\n",
    "transfer_summary = transfer_summary.withColumn(\n",
    "    \"total_transfer_fee\", F.col(\"total_transfer_fee_in\") - F.col(\"total_transfer_fee_out\")\n",
    ").withColumn(\n",
    "    \"total_market_value\", F.col(\"total_market_value_in\") - F.col(\"total_market_value_out\")\n",
    ")\n",
    "\n",
    "# Dropping intermediate columns (total_transfer_fee_out, total_transfer_fee_in, etc.)\n",
    "transfer_summary = transfer_summary.drop(\"total_transfer_fee_out\", \"total_transfer_fee_in\", \"total_market_value_out\", \"total_market_value_in\")\n",
    "\n",
    "# Convert season format (extract first two digits and convert to integer)\n",
    "transfer_summary = transfer_summary.withColumn(\n",
    "    \"season\", F.col(\"transfer_season\").substr(1, 2).cast(\"int\") + 2000\n",
    ")\n",
    "\n",
    "# Load the team stats data (assuming it's already loaded as df_team_stats)\n",
    "df_team_stats = spark.read.parquet(\"dbfs:/tmp/team_stats.parquet\")  # Replace with actual path\n",
    "\n",
    "# Merge the team stats with the transfer summary\n",
    "df_team_stats = df_team_stats.join(\n",
    "    transfer_summary,\n",
    "    on=[\"season\", \"club_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill any NaN values with 0\n",
    "df_team_stats = df_team_stats.fillna(0)\n",
    "\n",
    "# Sort data by club_id and season\n",
    "df_team_stats = df_team_stats.orderBy(\"club_id\", \"season\")\n",
    "\n",
    "# Cumulative sum for transfer_fee and market_value per club\n",
    "df_team_stats = df_team_stats.withColumn(\n",
    "    \"total_transfer_fee\", F.sum(\"total_transfer_fee\").over(Window.partitionBy(\"club_id\").orderBy(\"season\"))\n",
    ").withColumn(\n",
    "    \"total_market_value\", F.sum(\"total_market_value\").over(Window.partitionBy(\"club_id\").orderBy(\"season\"))\n",
    ")\n",
    "\n",
    "# Show the final result\n",
    "\n",
    "df_team_stats.write.mode(\"overwrite\").parquet(\"dbfs:/tmp/team_stats_with_transfers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bccb8d5e-f07a-4e45-b052-ad8e217d2688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_appearances2 = spark.read.parquet(\"dbfs:/tmp/appearances2.parquet\")\n",
    "df_clubs = spark.read.parquet(\"dbfs:/tmp/clubs.parquet\")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define Big Five leagues and seasons\n",
    "big_five_leagues = ['L1', 'GB1', 'ES1', 'FR1', 'IT1']\n",
    "start_date = '2014-07-01'\n",
    "end_date = '2024-06-30'\n",
    "\n",
    "# Filter data for the required leagues and seasons\n",
    "# Perform the join with clubs to get domestic_competition_id\n",
    "df_filtered = df_appearances2.join(\n",
    "    df_clubs, df_appearances2[\"player_club_id\"] == df_clubs[\"club_id\"], how=\"inner\"\n",
    ").filter(\n",
    "    (F.col(\"date\") >= start_date) &\n",
    "    (F.col(\"date\") <= end_date) &\n",
    "    (F.col(\"domestic_competition_id\").isin(big_five_leagues))\n",
    ")\n",
    "\n",
    "df_player_data = df_filtered.groupBy(\"player_id\").agg(\n",
    "    F.count(\"*\").alias(\"games_played\"),\n",
    "    F.sum(\"yellow_cards\").alias(\"total_yellow_cards\"),\n",
    "    F.sum(\"red_cards\").alias(\"total_red_cards\"),\n",
    "    F.sum(\"goals\").alias(\"total_goals\"),\n",
    "    F.sum(\"assists\").alias(\"total_assists\"),\n",
    "    F.sum(\"GF\").alias(\"total_goals_for_team\"),\n",
    "    F.sum(\"GA\").alias(\"total_goals_against_team\"),\n",
    "    (F.round(F.avg(\"yellow_cards\"), 2)).alias(\"average_yellow_cards\"),\n",
    "    (F.round(F.avg(\"red_cards\"), 2)).alias(\"average_red_cards\"),\n",
    "    (F.round(F.avg(\"goals\"), 2)).alias(\"average_goals\"),\n",
    "    (F.round(F.avg(\"assists\"), 2)).alias(\"average_assists\"),\n",
    "    (F.round(F.avg(\"GF\"), 2)).alias(\"average_goals_for_team\"),\n",
    "    (F.round(F.avg(\"GA\"), 2)).alias(\"average_goals_against_team\"),\n",
    "    (F.round(F.avg(\"minutes_played\"), 2)).alias(\"avg_minutes_played\"),\n",
    "    F.sum(F.when(F.col(\"result\") == 'W', 1).otherwise(0)).alias(\"total_wins\"),\n",
    "    F.sum(F.when(F.col(\"result\") == 'D', 1).otherwise(0)).alias(\"total_draws\"),\n",
    "    F.sum(F.when(F.col(\"result\") == 'L', 1).otherwise(0)).alias(\"total_losses\"),\n",
    "    (F.round(\n",
    "        (3.0 * F.sum(F.when(F.col(\"result\") == 'W', 1).otherwise(0)) + \n",
    "         1.0 * F.sum(F.when(F.col(\"result\") == 'D', 1).otherwise(0))) / \n",
    "        (3.0 * F.count(\"*\")) * 100, 2)\n",
    "    ).alias(\"performance\")\n",
    ")\n",
    "\n",
    "# Filter players who have played at least 100 games\n",
    "df_player_data = df_player_data.filter(F.col(\"games_played\") >= 100)\n",
    "df_players = spark.read.csv(\"/FileStore/tables/players.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_top_players = df_player_data.join(\n",
    "    df_players, df_player_data[\"player_id\"] == df_players[\"player_id\"], how=\"left\"\n",
    ").select(\n",
    "    df_player_data[\"player_id\"],\n",
    "    df_players[\"name\"].alias(\"player_name\"),\n",
    "    df_player_data[\"games_played\"],\n",
    "    df_player_data[\"performance\"],\n",
    "    df_player_data[\"total_goals\"],\n",
    "    df_player_data[\"total_assists\"],\n",
    "    df_player_data[\"total_yellow_cards\"],\n",
    "    df_player_data[\"total_red_cards\"],\n",
    "    df_player_data[\"total_goals_for_team\"],\n",
    "    df_player_data[\"total_goals_against_team\"],\n",
    "    df_player_data[\"average_yellow_cards\"],\n",
    "    df_player_data[\"average_red_cards\"],\n",
    "    df_player_data[\"average_goals\"],\n",
    "    df_player_data[\"average_assists\"],\n",
    "    df_player_data[\"average_goals_for_team\"],\n",
    "    df_player_data[\"average_goals_against_team\"],\n",
    "    df_player_data[\"avg_minutes_played\"],\n",
    "    df_player_data[\"total_wins\"],\n",
    "    df_player_data[\"total_draws\"],\n",
    "    df_player_data[\"total_losses\"]\n",
    ")\n",
    "\n",
    "# Sort by performance in descending order and get top 10 players\n",
    "df_top_players_10 = df_top_players.orderBy(F.col(\"performance\"), ascending=False).limit(10)\n",
    "df_top_score_10 = df_top_players.orderBy(F.col(\"total_goals\"), ascending=False).limit(10)\n",
    "\n",
    "df_top_players_10.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_players\")\n",
    "df_top_score_10.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_scorers\")\n",
    "\n",
    "# Show top players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b20ab14a-709a-41a1-9f30-8122797e8130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define Big Five leagues and seasons\n",
    "big_five_leagues = ['L1', 'GB1', 'ES1', 'FR1', 'IT1']\n",
    "seasons = list(range(2014, 2024))\n",
    "\n",
    "team_stats_with_transfers = spark.read.parquet(\"dbfs:/tmp/team_stats_with_transfers.parquet\")\n",
    "# Filter data for the required leagues and seasons\n",
    "df_top_teams = team_stats_with_transfers.filter(\n",
    "    (F.col(\"domestic_competition_id\").isin(big_five_leagues)) &\n",
    "    (F.col(\"season\").isin(seasons)) &\n",
    "    (F.col(\"total_games\") > 10)\n",
    ")\n",
    "\n",
    "# Select and sort by performance in descending order\n",
    "df_top_teams = df_top_teams.select(\n",
    "    \"season\",\n",
    "    \"club_id\",\n",
    "    \"club_name\",\n",
    "    F.col(\"domestic_competition_id\").alias(\"dom_league\"),\n",
    "    \"performance\",\n",
    "    \"total_games\",\n",
    "    \"win\",\n",
    "    \"draw\",\n",
    "    \"lost\",\n",
    "    \"GF\",\n",
    "    \"GA\",\n",
    "    \"GD\"\n",
    ").orderBy(F.col(\"performance\").desc())\n",
    "\n",
    "# Get the top 10 teams\n",
    "df_top_teams_by_performance_and_season = df_top_teams.limit(10)\n",
    "df_top_teams_by_performance_and_season.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_teams_by_performance_and_season\")\n",
    "# Show the result\n",
    "df_top_teams.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8a60ad-3dfa-4e61-8268-07486c3b7468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_stats  = spark.read.parquet(\"dbfs:/tmp/team_stats.parquet\")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Định nghĩa danh sách các giải đấu lớn\n",
    "big_five_leagues = ['L1', 'GB1', 'ES1', 'FR1', 'IT1']\n",
    "\n",
    "# Lọc dữ liệu theo giải đấu và số trận tối thiểu\n",
    "df_top_teams = team_stats.filter(\n",
    "    (F.col(\"domestic_competition_id\").isin(big_five_leagues)) &\n",
    "    (F.col(\"season\").between(2014, 2023)) &  # Giới hạn mùa giải từ 2014 đến 2023\n",
    "    (F.col(\"total_games\") > 10)  # Chỉ xét đội có hơn 10 trận đấu\n",
    ")\n",
    "\n",
    "# Tính tổng số trận, thắng, hòa, thua, số bàn thắng, số bàn thua\n",
    "df_top_teams = df_top_teams.groupBy(\n",
    "    \"club_id\", \"club_name\", \"domestic_competition_id\"\n",
    ").agg(\n",
    "    F.sum(\"total_games\").alias(\"total_games\"),\n",
    "    F.sum(\"win\").alias(\"win\"),\n",
    "    F.sum(\"draw\").alias(\"draw\"),\n",
    "    F.sum(\"lost\").alias(\"lost\"),\n",
    "    F.sum(\"GF\").alias(\"GF\"),\n",
    "    F.sum(\"GA\").alias(\"GA\"),\n",
    "    (F.sum(\"GF\") - F.sum(\"GA\")).alias(\"GD\"),\n",
    "    # Tính hiệu suất (performance) = ((Tổng điểm đạt được) / (Tổng điểm tối đa)) * 100\n",
    "    F.round(\n",
    "        ((F.sum(\"win\") * 3.0 + F.sum(\"draw\")) * 100.0) / (F.sum(\"total_games\") * 3.0), 2\n",
    "    ).alias(\"performance\")\n",
    ")\n",
    "\n",
    "# Sắp xếp theo hiệu suất giảm dần và lấy top 10\n",
    "df_top_teams_by_performance_last_decade = df_top_teams.orderBy(F.col(\"performance\").desc()).limit(10)\n",
    "df_top_teams_by_performance_last_decade.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_teams_by_performance_last_decade\")\n",
    "# Hiển thị kết quả\n",
    "# df_top_teams.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc58503-f1e1-415c-9842-fb6c86ae7d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# \"total_transfer_fee\", \"total_transfer_fee\"\n",
    "team_stats_with_transfers = spark.read.parquet(\"dbfs:/tmp/team_stats_with_transfers.parquet\")\n",
    "# Define Big Five leagues and seasons\n",
    "big_five_leagues = ['L1', 'GB1', 'ES1', 'FR1', 'IT1']\n",
    "seasons = list(range(2014, 2024))\n",
    "\n",
    "# Filter data for the required leagues and seasons\n",
    "df_top_teams = team_stats_with_transfers.filter(\n",
    "    (F.col(\"domestic_competition_id\").isin(big_five_leagues)) &\n",
    "    (F.col(\"season\").isin(seasons)) &\n",
    "    (F.col(\"total_games\") > 10)\n",
    ")\n",
    "\n",
    "# Select and sort by performance in descending order\n",
    "df_top_teams = df_top_teams.select(\n",
    "    \"season\",\n",
    "    \"club_id\",\n",
    "    \"club_name\",\n",
    "    F.col(\"domestic_competition_id\").alias(\"dom_league\"),\n",
    "    \"performance\",\n",
    "    \"total_games\",\n",
    "    \"total_market_value\",\n",
    ").orderBy(F.col(\"total_market_value\").desc())\n",
    "\n",
    "# Get the top 10 teams\n",
    "df_top_teams_by_market_value_and_season = df_top_teams.limit(10)\n",
    "\n",
    "df_top_teams_by_market_value_and_season.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_teams_by_market_value_and_season\")\n",
    "# Show the result\n",
    "# df_top_teams_10.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2068a315-ed57-44fe-a921-f5e34935c653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_top_teams is already a PySpark DataFrame, let's convert it to Pandas for plotting\n",
    "df = df_top_teams.toPandas()  # Convert PySpark DataFrame to Pandas DataFrame\n",
    "\n",
    "# Prepare the data\n",
    "X = df[['total_market_value']].values  # Independent variable (market value)\n",
    "y = df['performance'].values  # Dependent variable (performance)\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the values using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the R² value (coefficient of determination)\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "# Plot the scatter plot and regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['total_market_value'], y=df['performance'], color='blue', alpha=0.6, label='Data points')\n",
    "\n",
    "# Plot the regression line\n",
    "sns.lineplot(x=df['total_market_value'], y=y_pred, color='red', label=f\"Trend line (R² = {r2:.2f})\")\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title(f\"Market Value vs Performance (R² = {r2:.2f})\")\n",
    "plt.xlabel(\"Market Value diff (M€)\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8771ba4-7f94-43ec-87e8-bbdf2cf55cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "team_stats_with_transfers = spark.read.parquet(\"dbfs:/tmp/team_stats_with_transfers.parquet\")\n",
    "# Define the valid seasons\n",
    "valid_seasons = list(range(2010, 2024))\n",
    "\n",
    "# Filter data according to the given criteria\n",
    "filtered_teams = team_stats_with_transfers.filter(\n",
    "    (F.col('domestic_competition_id').isin(['L1', 'GB1', 'ES1', 'FR1', 'IT1'])) &\n",
    "    (F.col('season').isin(valid_seasons)) &\n",
    "    (F.col('total_games') > 10)\n",
    ")\n",
    "\n",
    "# # Select and sort by performance in descending order\n",
    "# df_top_teams = df_top_teams.select(\n",
    "#     \"season\",\n",
    "#     \"club_id\",\n",
    "#     \"club_name\",\n",
    "#     F.col(\"domestic_competition_id\").alias(\"dom_league\"),\n",
    "#     \"performance\",\n",
    "#     \"total_games\",\n",
    "#     \"total_market_value\",\n",
    "# ).orderBy(F.col(\"total_market_value\").desc())\n",
    "\n",
    "# Group by club_id and calculate the necessary aggregations\n",
    "top_market_value = filtered_teams.groupBy(\n",
    "    'club_id', 'club_name', 'domestic_competition_id'\n",
    ").agg(\n",
    "    F.sum('total_market_value').alias('total_market_value'),\n",
    "    # Calculate performance as specified\n",
    "    F.round(\n",
    "        (F.sum('win') * 3.0 + F.sum('draw')) * 100.0 / (F.sum('total_games') * 3.0), 2\n",
    "    ).alias('performance'),\n",
    "    F.sum('total_games').alias('total_games'),\n",
    "    F.sum('win').alias('win'),\n",
    "    F.sum('draw').alias('draw'),\n",
    "    F.sum('lost').alias('lost'),\n",
    "    F.sum('GF').alias('GF'),\n",
    "    F.sum('GA').alias('GA')\n",
    ")\n",
    "\n",
    "# Calculate the goal difference (GD)\n",
    "top_market_value = top_market_value.withColumn(\n",
    "    'GD', F.col('GF') - F.col('GA')\n",
    ")\n",
    "\n",
    "# Sort by total_market_value and get the top 10 teams\n",
    "df_top_teams_by_market_value_last_decade = top_market_value.select(\n",
    "    \"club_id\",\n",
    "    \"club_name\",\n",
    "    F.col(\"domestic_competition_id\").alias(\"dom_league\"),\n",
    "    \"performance\",\n",
    "    \"total_games\",\n",
    "    \"total_market_value\",\n",
    ").orderBy(F.col('total_market_value'), ascending=False).limit(10)\n",
    "\n",
    "df_top_teams_by_market_value_last_decade.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_database.top_teams_by_market_value_last_decade\")\n",
    "# Show the top 10 teams\n",
    "# top_market_value.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5f851e-7f0c-4985-b0b6-39ca8b81f79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_top_teams is already a PySpark DataFrame, let's convert it to Pandas for plotting\n",
    "df = top_market_value.toPandas()  # Convert PySpark DataFrame to Pandas DataFrame\n",
    "df = df[df['total_market_value'] > 0].copy()\n",
    "# Tạo mô hình hồi quy tuyến tính\n",
    "X = df[['total_market_value']]\n",
    "y = df['performance'].values  # Dependent variable (performance)\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the values using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the R² value (coefficient of determination)\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "# Plot the scatter plot and regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df['total_market_value'], y=df['performance'], color='blue', alpha=0.6, label='Data points')\n",
    "\n",
    "# Plot the regression line\n",
    "sns.lineplot(x=df['total_market_value'], y=y_pred, color='red', label=f\"Trend line (R² = {r2:.2f})\")\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title(f\"Market Value vs Performance (R² = {r2:.2f})\")\n",
    "plt.xlabel(\"Market Value diff (M€)\")\n",
    "plt.ylabel(\"Performance (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb16c12a-4b75-4b92-bdb2-11904af3ce7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS my_database;\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6853877528013976,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "For \"Big-Data\" project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
